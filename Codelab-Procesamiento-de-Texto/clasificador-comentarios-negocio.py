# Requisitos (si falta): pip install scikit-learn pandas numpy joblib matplotlib

import re, random, numpy as np, pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.pipeline import make_pipeline
import joblib

# ----------------------------
# 1) Dataset sint칠tico realista
# ----------------------------
random.seed(42); np.random.seed(42) # Explicacion detallada: Esto asegura que los resultados sean reproducibles al fijar la semilla para las funciones aleatorias de Python y NumPy.

positivos = [
    "Excelente servicio","Muy buena atenci칩n","Me encant칩 el producto",
    "R치pido y confiable","Todo lleg칩 perfecto","Calidad superior",
    "Lo recomiendo totalmente","Volver칠 a comprar","Precio justo y buena calidad",
    "El soporte fue amable","Experiencia incre칤ble","Funcion칩 mejor de lo esperado",
    "Entregado a tiempo","Muy satisfecho","Cinco estrellas",
    "La comida estaba deliciosa","El empaque impecable","S칰per recomendable",
    "Buen trato del personal","Gran experiencia"
]

negativos = [
    "P칠simo servicio","Muy mala atenci칩n","Odio este producto",
    "Lento y poco confiable","Lleg칩 da침ado","Calidad terrible",
    "No lo recomiendo","No vuelvo a comprar","Caro y mala calidad",
    "El soporte fue grosero","Experiencia horrible","Peor de lo esperado",
    "Entregado tarde","Muy decepcionado","Una estrella",
    "La comida estaba fr칤a","El empaque roto","Nada recomendable",
    "Mal trato del personal","Mala experiencia"
]

def variantes(frase):
    extras = ["", "!", "!!", " 游뗵", " 游땨", " de verdad", " en serio", " 10/10", " 1/10",
              " s칰per", " la verdad", " jam치s", " nunca", " para nada"]
    return frase + random.choice(extras)

pos = [variantes(p) for _ in range(8) for p in positivos] # Aumentar datos con variantes
neg = [variantes(n) for _ in range(8) for n in negativos] # Aumentar datos con variantes
textos = pos + neg # Mezclar positivos y negativos
etiquetas = [1]*len(pos) + [0]*len(neg) # 1=Positivo, 0=Negativo

df = pd.DataFrame({"texto": textos, "etiqueta": etiquetas}).sample(frac=1, random_state=42).reset_index(drop=True)
# Mezclar filas aleatoriamente

print("Muestras:", df.shape[0], " | Positivos:", df.etiqueta.sum(), " | Negativos:", len(df)-df.etiqueta.sum())

# ----------------------------
# 2) Limpieza simple
# ----------------------------
def limpiar(s: str) -> str:
    s = s.lower()
    s = re.sub(r"[^a-z치칠칤칩칰침칲0-9\s]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

df["texto_clean"] = df["texto"].apply(limpiar)

# ----------------------------
# 3) Split estratificado + baseline
# ----------------------------
X_train_text, X_test_text, y_train, y_test = train_test_split(
    df["texto_clean"], df["etiqueta"], test_size=0.2, random_state=42, stratify=df["etiqueta"]
) # Mantener proporci칩n de clases en train/test

# Baseline: predecir siempre la clase mayoritaria (en este caso, positivo=1 o negativo=0)
mayoritaria = int(round(y_train.mean()))  # 0 o 1
baseline = (y_test == mayoritaria).mean() # Proporci칩n de la clase mayoritaria en test
print(f"Baseline (clase mayoritaria): {baseline:.3f}") # Ej: 0.505 si clases balanceadas

# ----------------------------
# 4) Vectorizador + SVM (entrenamiento)
# ----------------------------
vectorizer = TfidfVectorizer(max_features=30000, ngram_range=(1,2), min_df=2)
# Unigrams + Bigrams, m칤nimo 2 apariciones,
# Un unigram es una sola palabra, un bigram es una secuencia de dos palabras consecutivas
# features es el n칰mero m치ximo de caracter칤sticas (palabras o combinaciones) a considerar
# min_df es la frecuencia m칤nima de documentos para que una caracter칤stica sea incluida en el vocabulario.

X_train = vectorizer.fit_transform(X_train_text) # Aprender vocabulario y transformar train es decir, convertir texto a matriz dispersa
# Una matriz dispersa es una representaci칩n eficiente de datos donde la mayor칤a de los elementos son cero, com칰n en procesamiento de texto.
X_test  = vectorizer.transform(X_test_text)
# OJO: solo transformamos test, no fit (no aprender vocabulario nuevo)

clf = LinearSVC(class_weight="balanced", random_state=42)  # SVM lineal con ajuste para clases desbalanceadas (es decir, si hay m치s positivos que negativos o viceversa)
# class_weight="balanced" ajusta autom치ticamente los pesos inversamente proporcionales a las frecuencias de las clases en los datos de entrada.
# Esto ayuda a manejar situaciones donde una clase es mucho m치s frecuente que otra, evitando que el modelo se sesgue hacia la clase mayoritaria.
# random_state=42 para reproducibilidad, es decir, para que los resultados sean consistentes en diferentes ejecuciones.
clf.fit(X_train, y_train) # Entrenar el clasificador SVM con los datos de entrenamiento

# ----------------------------
# 5) Evaluaci칩n clara
# ----------------------------
pred = clf.predict(X_test) # Predecir etiquetas en test
acc = accuracy_score(y_test, pred) # Accuracy = (TP+TN) / (TP+TN+FP+FN)
print(f"\nAccuracy en test: {acc:.3f}  |  Mejora vs baseline: {acc - baseline:.3f}\n")
print("Reporte por clase:")
print(classification_report(y_test, pred, digits=3)) # Precisi칩n, Recall, F1-score por clase

cm = confusion_matrix(y_test, pred, labels=[0,1]) # Matriz de confusi칩n
print("\nMatriz de confusi칩n:")
print(pd.DataFrame(cm, index=["Real 0 (neg)", "Real 1 (pos)"], columns=["Pred 0 (neg)", "Pred 1 (pos)"]))
# Visualizaci칩n simple de la matriz de confusi칩n


# ----------------------------
# 6) Validaci칩n cruzada SIN tocar el vectorizer entrenado
#    (usamos un Pipeline independiente)
# ----------------------------
pipe = make_pipeline( # Un pipeline es una secuencia de pasos que se aplican a los datos de manera ordenada.
    TfidfVectorizer(max_features=30000, ngram_range=(1,2), min_df=2),
    LinearSVC(class_weight="balanced", random_state=42)
)
scores = cross_val_score(pipe, df["texto_clean"], df["etiqueta"], cv=5, scoring="f1_macro")  # cross_val_score realiza validaci칩n cruzada
print(f"\nCV (5-fold) F1_macro: media={scores.mean():.3f}  췀{scores.std():.3f}")

# ----------------------------
# 7) Predicci칩n de textos NUEVOS (vida real)
# ----------------------------
def predecir(textos_nuevos):
    tx = [limpiar(t) for t in textos_nuevos]
    Xn = vectorizer.transform(tx)  # 춰OJO! usamos el MISMO vectorizer entrenado
    p = clf.predict(Xn)
    return ["positivo" if i==1 else "negativo" for i in p]

nuevos = [
    "El env칤o fue rapid칤simo y el empaque lleg칩 impecable, gracias!",
    "Demoraron demasiado y adem치s nadie respondi칩 los mensajes",
    "Calidad/precio brutal, qued칠 muy satisfecho",
    "No lo recomiendo, sali칩 defectuoso y me toc칩 devolverlo"
]
print("\nPredicciones en textos nuevos:")
for t, etiqueta in zip(nuevos, predecir(nuevos)):
    print(f"- {t}  ->  {etiqueta}")

# ----------------------------
# 8) Guardar y cargar (producci칩n)
# ----------------------------
joblib.dump(vectorizer, "tfidf.joblib")
joblib.dump(clf, "modelo.joblib")
print("\nModelo y vectorizador guardados.")

# Ejemplo de carga y uso:
vec = joblib.load("tfidf.joblib")
model = joblib.load("modelo.joblib")
Xn = vec.transform(["pesimo"])
print("Pred loaded model:", "positivo" if model.predict(Xn)[0]==1 else "negativo")